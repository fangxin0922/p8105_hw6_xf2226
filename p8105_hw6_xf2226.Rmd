---
title: "p8105_hw6_xf2226"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(modelr)
library(mgcv)
library(patchwork)
library(knitr)
library(MASS)

theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis",
  digits = 3
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d 
knitr::opts_chunk$set(
  fig.height = 6,
  fig.width = 8,
  message = F,
  warning = F 
  )
```


# Q1
## Load and clean dataset
```{r, load and clean the data}
bt_df = 
  read_csv("./data/birthweight.csv") %>% 
  janitor::clean_names() %>% 
  mutate(
    babysex = factor(babysex, levels = c('1', '2'), labels = c("male", "female")),
    malform = factor(malform, levels = c('0', '1'), labels = c("absent", "present")),
    frace = factor(frace, levels = c("1", "2", "3", "4", "8", "9"), labels = c("White", "Black", "Asian", "Puerto Rican", "Other", "Unknown")),
    mrace = factor(mrace, levels = c("1", "2", "3", "4", "8", "9"), labels = c("White", "Black", "Asian", "Puerto Rican", "Other", "Unknown")))
```

Based on a literature for the factors that underly birthweight, we might include: 

* babysex: baby's sex - male babies are usually heavier
* blength: baby’s length at birth (centimeters) - taller baby may be heavier just because their height
* bhead: baby’s head circumference at birth (centimeters) - babies with bigger head circumference may be heavier too
* momage: mother’s age at delivery (years) - younger mother may be able to give birth to heathier and heavier babied compared to mother over age 35.
* gaweeks: gestational age in weeks - babies with longer gestational age may be heavier and heathier. 

```{r, my regression model based on literature}
my_model = lm(bwt ~ babysex + gaweeks + blength + bhead + momage, data = bt_df) 
summary(my_model)
```
All variables are significant. I will likely keep all variables in my final model. 

Based on a data-driven model-building process
```{r, my regression model based on data-driven process}
full.model <- lm(bwt  ~., data = bt_df)
step.model <- stepAIC(full.model, direction = "backward", 
                      trace = FALSE)
summary(step.model)
```
The significant predictors are babysex, bhead, blength, delwt, parity, mheight, mrace, ppwt, and smoken. A lot of these variable are also in the hypothesized structure driven model 

```{r, my final model}
my_model = lm(bwt ~ babysex + bhead + blength + delwt + momage + parity + gaweeks + mheight + mrace + ppwt + smoken, data = bt_df)
```

## Plotting the final model
```{r}
bt_df %>% 
  add_predictions(my_model) %>% 
  add_residuals(my_model) %>% 
  ggplot(aes(x = pred, y = resid)) + 
  geom_point(alpha = .3) +
    labs(
    title = "Plot of Models Residuals against Fitted Values",
    x = "Predicted Value",
    y = "Residual"
  )
```

## Compared models
Compare my model to 1) a model using length at birth and gestational age as predictors 2) a model using head circumference, length, sex and all interactions between these. 

```{r}
main_model = lm(bwt ~ blength + gaweeks, data = bt_df)
interaction_model = lm(bwt ~ bhead * babysex * blength, data = bt_df) 
```

We will use 80% of the data for our training and 20% for our testing.

```{r}
bt_df = bt_df %>%
  mutate(ID = row_number())
train_df = sample_n(bt_df, 3474)
test_df = anti_join(bt_df, train_df, by = "ID")
```

Let's compare the distributions of the training and testing datasets using bwt and blength. 

```{r}
ggplot(train_df, aes(x = blength, y = bwt)) + 
  geom_point() + 
  geom_point(data = test_df, colour = "red")
```

The distributions seems quite similar. 

Fit each of three models to the training data. 

```{r}
my_model = lm(bwt ~ babysex + bhead + blength + delwt + fincome + 
    gaweeks + mheight + mrace + parity + ppwt + smoken, data = train_df)
model1 = lm(bwt ~ blength + gaweeks, data = train_df)
model2 = lm(bwt ~ bhead + blength + babysex + bhead:blength + bhead:babysex + blength:babysex + bhead:blength:babysex, data = train_df)
```

## Compute and compare root mean squared errors (RMSEs).

```{r}
rmse(my_model, test_df)
rmse(model1, test_df)
rmse(model2, test_df)
```
The RMSEs suggest that my_model works the best.

## Final model comparison with more training sets 
```{r}
cv_df = 
  crossv_mc(bt_df, 100) #create 100 training sets. 
cv_df = 
  cv_df %>% 
  mutate(
    my_model  = map(.x = train, ~lm(bwt ~ babysex + bhead + blength + delwt + fincome + 
    gaweeks + mheight + mrace + parity + ppwt + smoken, data = .x))
    ) %>% 
  mutate(
    rmse_my_model = map2_dbl(my_model, test, ~rmse(model = .x, data = .y))
    )
cv_df = 
  cv_df %>% 
  mutate(
    main_model = map(.x = train, ~lm(bwt ~ blength + gaweeks, data = .x))
    ) %>% 
  mutate(
    rmse_main_model = map2_dbl(main_model, test, ~rmse(model = .x, data = .y))
    )
cv_df = 
  cv_df %>% 
  mutate(
    interaction_model = map(.x = train, ~lm(bwt ~ bhead * babysex * blength, data = .x))
    ) %>% 
  mutate(
    rmse_interaction_model = map2_dbl(interaction_model, test, ~rmse(model = .x, data = .y))
    )
```

## Prediction error distribution
```{r}
cv_df %>% 
  dplyr::select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + geom_violin() + labs(
    x = "Model",
    y = "RMSE", 
    title = "prediction error distribution"
  )
```

According on the graphs, it appears that my model is the best, the second best is main model, then interaction model.

# Question 2
## Load weather Data
```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  dplyr::select(name, id, everything())
```

