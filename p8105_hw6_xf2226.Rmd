---
title: "p8105_hw6_xf2226"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(modelr)
library(mgcv)
library(patchwork)
library(knitr)
library(MASS)

theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis",
  digits = 3
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d 
knitr::opts_chunk$set(
  fig.height = 6,
  fig.width = 8,
  message = F,
  warning = F 
  )
```


# Q1
## Load and clean dataset
```{r, load and clean the data}
bt_df = 
  read_csv("./data/birthweight.csv") %>% 
  janitor::clean_names() %>% 
  mutate(
    babysex = factor(babysex, levels = c('1', '2'), labels = c("male", "female")),
    malform = factor(malform, levels = c('0', '1'), labels = c("absent", "present")),
    frace = factor(frace, levels = c("1", "2", "3", "4", "8", "9"), labels = c("White", "Black", "Asian", "Puerto Rican", "Other", "Unknown")),
    mrace = factor(mrace, levels = c("1", "2", "3", "4", "8", "9"), labels = c("White", "Black", "Asian", "Puerto Rican", "Other", "Unknown")))
```

Based on a literature for the factors that underly birthweight, we might include: 

* babysex: baby's sex - male babies are usually heavier
* blength: baby’s length at birth (centimeters) - taller baby may be heavier just because their height
* bhead: baby’s head circumference at birth (centimeters) - babies with bigger head circumference may be heavier too
* momage: mother’s age at delivery (years) - younger mother may be able to give birth to heathier and heavier babied compared to mother over age 35.
* gaweeks: gestational age in weeks - babies with longer gestational age may be heavier and heathier. 

```{r, my regression model based on literature}
my_model = lm(bwt ~ babysex + gaweeks + blength + bhead + momage, data = bt_df) 
summary(my_model)
```
All variables are significant. I will likely keep all variables in my final model. 

Based on a data-driven model-building process
```{r, my regression model based on data-driven process}
full.model <- lm(bwt  ~., data = bt_df)
step.model <- stepAIC(full.model, direction = "backward", 
                      trace = FALSE)
summary(step.model)
```
The significant predictors are babysex, bhead, blength, delwt, parity, mheight, mrace, ppwt, and smoken. A lot of these variable are also in the hypothesized structure driven model 

```{r, my final model}
my_model = lm(bwt ~ babysex + bhead + blength + delwt + momage + parity + gaweeks + mheight + mrace + ppwt + smoken, data = bt_df)
```

## Plotting the final model
```{r}
bt_df %>% 
  add_predictions(my_model) %>% 
  add_residuals(my_model) %>% 
  ggplot(aes(x = pred, y = resid)) + 
  geom_point(alpha = .3) +
    labs(
    title = "Plot of Models Residuals against Fitted Values",
    x = "Predicted Value",
    y = "Residual"
  )
```

## Compared models
Compare my model to 1) a model using length at birth and gestational age as predictors 2) a model using head circumference, length, sex and all interactions between these. 

```{r}
main_model = lm(bwt ~ blength + gaweeks, data = bt_df)
interaction_model = lm(bwt ~ bhead * babysex * blength, data = bt_df) 
```

We will use 80% of the data for our training and 20% for our testing.

```{r}
bt_df = bt_df %>%
  mutate(ID = row_number())
train_df = sample_n(bt_df, 3474)
test_df = anti_join(bt_df, train_df, by = "ID")
```

Let's compare the distributions of the training and testing datasets using bwt and blength. 

```{r}
ggplot(train_df, aes(x = blength, y = bwt)) + 
  geom_point() + 
  geom_point(data = test_df, colour = "red")
```

The distributions seems quite similar. 

Fit each of three models to the training data. 

```{r}
my_model = lm(bwt ~ babysex + bhead + blength + delwt + fincome + 
    gaweeks + mheight + mrace + parity + ppwt + smoken, data = train_df)
model1 = lm(bwt ~ blength + gaweeks, data = train_df)
model2 = lm(bwt ~ bhead + blength + babysex + bhead:blength + bhead:babysex + blength:babysex + bhead:blength:babysex, data = train_df)
```

## Compute and compare root mean squared errors (RMSEs).

```{r}
rmse(my_model, test_df)
rmse(model1, test_df)
rmse(model2, test_df)
```
The RMSEs suggest that my_model works the best.

## Final model comparison with more training sets 
```{r}
cv_df = 
  crossv_mc(bt_df, 100) #create 100 training sets. 
cv_df = 
  cv_df %>% 
  mutate(
    my_model  = map(.x = train, ~lm(bwt ~ babysex + bhead + blength + delwt + fincome + 
    gaweeks + mheight + mrace + parity + ppwt + smoken, data = .x))
    ) %>% 
  mutate(
    rmse_my_model = map2_dbl(my_model, test, ~rmse(model = .x, data = .y))
    )
cv_df = 
  cv_df %>% 
  mutate(
    main_model = map(.x = train, ~lm(bwt ~ blength + gaweeks, data = .x))
    ) %>% 
  mutate(
    rmse_main_model = map2_dbl(main_model, test, ~rmse(model = .x, data = .y))
    )
cv_df = 
  cv_df %>% 
  mutate(
    interaction_model = map(.x = train, ~lm(bwt ~ bhead * babysex * blength, data = .x))
    ) %>% 
  mutate(
    rmse_interaction_model = map2_dbl(interaction_model, test, ~rmse(model = .x, data = .y))
    )
```

## Prediction error distribution
```{r}
cv_df %>% 
  dplyr::select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + geom_violin() + labs(
    x = "Model",
    y = "RMSE", 
    title = "prediction error distribution"
  )
```

According on the graphs, it appears that my model is the best, the second best is main model, then interaction model.

# Q2
## Load weather Data
```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  dplyr::select(name, id, everything())
```

## Create Bootsrap samples
```{r, creating bootstrap samples}
set.seed(2110)
boot_sample = function(df) {
  sample_frac(df, replace = TRUE)
}
boot_straps = 
  tibble(
    strap_number = 1:5000,
    strap_sample = rerun(5000, boot_sample(weather_df))
  ) 
```

## Create Bootsrap results
```{r, bootstrap results}
bootstrap_results = 
  boot_straps %>% 
  mutate(
    models = map(strap_sample, ~lm(tmax ~ tmin, data = .x)),
    results = map(models, broom::tidy)) %>% 
  dplyr::select(-strap_sample, -models) %>% 
  unnest(results) 

bootstrap_results2 = 
  boot_straps %>% 
  mutate(
    models = map(strap_sample, ~lm(tmax ~ tmin, data = .x)),
    results = map(models, broom::glance)) %>% 
  dplyr::select(-strap_sample, -models) %>% 
  unnest(results) %>% 
  dplyr::select(strap_number, r.squared)

final_bootstrap_results =
  bootstrap_results %>% 
    mutate(log_estimate = log(estimate)) %>% 
    group_by(strap_number) %>% 
    summarise(log_b0_b1 = sum(log_estimate)) 

final_bootstrap_results = 
  merge(final_bootstrap_results, bootstrap_results2, by = "strap_number")
```

## Plotting the Estimates
```{r}
log_b0_b1_plot = 
final_bootstrap_results %>% 
  ggplot(aes(x = log_b0_b1)) + 
  geom_density() +
  labs(
   x = "Log of b0*b1 estimate distribution",
   y = "Density")
r_squared_plot =
final_bootstrap_results %>% 
  ggplot(aes(x = r.squared)) +
  geom_density() +
  labs(
   x = "R-squared estimate distribution",
   y = "Density")
log_b0_b1_plot / r_squared_plot
```
From the plots above, we can see that the distribution of the $\hat{r}^{2}$ estimate and the distribution of the Estimated $\log(\hat{\beta}_{0} * \hat{\beta}_{1})$ follow roughly a normal distribution. The Estimated $\log(\hat{\beta}_{0} * \hat{\beta}_{1})$ distribution was centered around 2.02.The $\hat{r}^{2}$ Estimate distribution was centered around 0.915.

## 95% CIs 
```{r}
ci_log_b0_b1 = 
  final_bootstrap_results %>% 
  summarize(
    ci_lower = quantile(log_b0_b1, 0.025), 
    ci_upper = quantile(log_b0_b1, 0.975))
ci_r2 = 
  final_bootstrap_results %>% 
  summarize(
    ci_lower = quantile(r.squared, 0.025), 
    ci_upper = quantile(r.squared, 0.975))
```
The 95% confidence interval for $\hat{r}^{2}$ is `r pull(ci_r2, ci_lower)` to `r pull(ci_r2, ci_upper)`. And the 95% confidence interval for $\log(\hat{\beta}_{0} * \hat{\beta}_{1})$ is `r pull(ci_log_b0_b1, ci_lower)` to `r pull(ci_log_b0_b1, ci_upper)`

